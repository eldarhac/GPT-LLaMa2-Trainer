# GPT-LLaMA2 Trainer

This project contains a modular pipeline for fine-tuning LLaMA2-based language models using Hugging Face's `transformers`, `peft`, and `trl` libraries.

## ğŸ“¦ Features

- Parameter-efficient fine-tuning (LoRA)
- Hugging Face Dataset loading
- OpenAI integration (optional)
- Supports Colab and local environments

## ğŸ“ Directory Structure

```
gpt_llama2_trainer/
â”‚
â”œâ”€â”€ gpt_llama2_trainer/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trainer.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Installation

```bash
git clone <repo-url>
cd gpt_llama2_trainer
pip install -r requirements.txt
```

### Usage

```bash
python scripts/train.py
```

## ğŸ§  Notes

- Colab-specific code is present; modify or remove for production environments.
- Make sure to configure datasets and model paths as needed in `trainer.py`.

---

Made with â¤ï¸ by Eldar Hacohen   
